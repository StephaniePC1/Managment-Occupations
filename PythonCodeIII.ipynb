{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PythonCodeIII.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMza6zohMpwYr2lBHSrs0FU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StephaniePC1/ThisIsWhatIDoNow/blob/master/PythonCodeIII.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-u7-kwR34wC4"
      },
      "source": [
        "# Regular Expression Basics\n",
        "# The Regular Expression Module\n",
        "import re\n",
        "\n",
        "titles = hn[\"title\"].tolist()\n",
        "python_mentions = 0\n",
        "pattern = \"[Pp]ython\"\n",
        "\n",
        "for t in titles:\n",
        "    if re.search(pattern, t):\n",
        "        python_mentions += 1\n",
        "------------------------------------\n",
        "# Counting Matches with pandas Methods\n",
        "pattern = '[Pp]ython'\n",
        "titles = hn['title']\n",
        "python_mentions = titles.str.contains(pattern).sum()\n",
        "----------------------------------------------------\n",
        "# 4.Using Regular Expressions to Select Data\n",
        "titles = hn['title']\n",
        "ruby_titles = titles[titles.str.contains(r\"[Rr]uby\")]\n",
        "---------------------------------------------------\n",
        "# 5.Quantifiers\n",
        "email_bool = titles.str.contains(\"e-?mail\")\n",
        "email_count = email_bool.sum()\n",
        "email_titles = titles[email_bool]\n",
        "--------------------------------------\n",
        "# 6.Character Classes\n",
        "pattern = \"\\[\\w+\\]\"\n",
        "tag_titles = titles[titles.str.contains(pattern)]\n",
        "tag_count = tag_titles.shape[0]\n",
        "------------------------------------------------\n",
        "# Accessing the Matching Text with Capture Groups\n",
        "pattern = r\"\\[(\\w+)\\]\"\n",
        "tag_freq = titles.str.extract(pattern).value_counts()\n",
        "--------------------------------------------------\n",
        "# Negative Character Classes\n",
        "def first_10_matches(pattern):\n",
        "    \"\"\"\n",
        "    Return the first 10 story titles that match\n",
        "    the provided regular expression\n",
        "    \"\"\"\n",
        "    all_matches = titles[titles.str.contains(pattern)]\n",
        "    first_10 = all_matches.head(10)\n",
        "    return first_10\n",
        "pattern = r\"[Jj]ava[^Ss]\"\n",
        "java_titles = titles[titles.str.contains(pattern)]\n",
        "-----------------------------------------------\n",
        "# Word Boundaries\n",
        "pattern = r\"\\b[Jj]ava\\b\"\n",
        "java_titles = titles[titles.str.contains(pattern)]\n",
        "----------------------------------------------\n",
        "# Matching at the Start and End of Strings\n",
        "pattern_beginning = r\"^\\[\\w+\\]\"\n",
        "beginning_count = titles.str.contains(pattern_beginning).sum()\n",
        "\n",
        "pattern_ending =  r\"\\[\\w+\\]$\"\n",
        "ending_count = titles.str.contains(pattern_ending).sum()\n",
        "-----------------------------------------------\n",
        "# Challenge: Using Flags to Modify Regex Patterns\n",
        "import re\n",
        "\n",
        "email_tests = pd.Series(['email', 'Email', 'e Mail', 'e mail', 'E-mail',\n",
        "              'e-mail', 'eMail', 'E-Mail', 'EMAIL', 'emails', 'Emails',\n",
        "              'E-Mails'])\n",
        "pattern = r\"\\be[\\-\\s]?mails?\\b\"\n",
        "email_mentions = titles.str.contains(pattern, flags=re.I).sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oEeVXyq6fsO"
      },
      "source": [
        "# Advanced Regular Expressions\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "hn = pd.read_csv(\"hacker_news.csv\")\n",
        "titles = hn['title']\n",
        "sql_pattern = r\"SQL\"\n",
        "sql_counts = titles.str.contains(sql_pattern, flags=re.I).sum()\n",
        "--------------------------------------------------\n",
        "# Capture Groups\n",
        "hn_sql = hn[hn['title'].str.contains(r\"\\w+SQL\", flags=re.I)].copy()\n",
        "hn_sql[\"flavor\"] = hn_sql[\"title\"].str.extract(r\"(\\w+SQL)\", re.I)\n",
        "hn_sql[\"flavor\"] = hn_sql[\"flavor\"].str.lower()\n",
        "sql_pivot = hn_sql.pivot_table(index=\"flavor\",values=\"num_comments\", aggfunc='mean')\n",
        "-------------------------------------------------\n",
        "# Using Capture Groups to Extract Data\n",
        "pattern = r\"[Pp]ython ([\\d\\.]+)\"\n",
        "\n",
        "py_versions = titles.str.extract(pattern)\n",
        "py_versions_freq = dict(py_versions.value_counts())\n",
        "-------------------------------------------------\n",
        "# Counting Mentions of the 'C' Language\n",
        "def first_10_matches(pattern):\n",
        "    \"\"\"\n",
        "    Return the first 10 story titles that match\n",
        "    the provided regular expression\n",
        "    \"\"\"\n",
        "    all_matches = titles[titles.str.contains(pattern)]\n",
        "    first_10 = all_matches.head(10)\n",
        "    return first_10\n",
        "\n",
        "# pattern = r\"\\b[Cc]\\b\"\n",
        "pattern = r\"\\b[Cc]\\b[^.+]\"\n",
        "first_ten = first_10_matches(pattern)\n",
        "----------------------------------------------------\n",
        "# Using Lookarounds to Control Matches Based on Surrounding Text\n",
        "pattern = r\"(?<!Series\\s)\\b[Cc]\\b(?![\\+\\.])\"\n",
        "c_mentions = titles.str.contains(pattern).sum()\n",
        "-------------------------------------------------------\n",
        "# BackReferences: Using Capture Groups in a RegEx Pattern\n",
        "pattern = r\"\\b(\\w+)\\s\\1\\b\"\n",
        "\n",
        "repeated_words = titles[titles.str.contains(pattern)]\n",
        "----------------------------------------------------\n",
        "# Substituting Regular Expression Matches\n",
        "email_variations = pd.Series(['email', 'Email', 'e Mail',\n",
        "                        'e mail', 'E-mail', 'e-mail',\n",
        "                        'eMail', 'E-Mail', 'EMAIL'])\n",
        "pattern = r\"\\be[-\\s]?mail\"\n",
        "email_uniform = email_variations.str.replace(pattern, \"email\", flags=re.I)\n",
        "titles_clean = titles.str.replace(pattern, \"email\", flags=re.I)\n",
        "----------------------------------------------------\n",
        "# Extracting Domains from URLs\n",
        "test_urls = pd.Series([\n",
        " 'https://www.amazon.com/Technology-Ventures-Enterprise-Thomas-Byers/dp/0073523429',\n",
        " 'http://www.interactivedynamicvideo.com/',\n",
        " 'http://www.nytimes.com/2007/11/07/movies/07stein.html?_r=0',\n",
        " 'http://evonomics.com/advertising-cannot-maintain-internet-heres-solution/',\n",
        " 'HTTPS://github.com/keppel/pinn',\n",
        " 'Http://phys.org/news/2015-09-scale-solar-youve.html',\n",
        " 'https://iot.seeed.cc',\n",
        " 'http://www.bfilipek.com/2016/04/custom-deleters-for-c-smart-pointers.html',\n",
        " 'http://beta.crowdfireapp.com/?beta=agnipath',\n",
        " 'https://www.valid.ly?param',\n",
        " 'http://css-cursor.techstream.org'\n",
        "])\n",
        "pattern = r\"https?://([\\w\\-\\.]+)\"\n",
        "\n",
        "test_urls_clean = test_urls.str.extract(pattern, flags=re.I)\n",
        "domains = hn['url'].str.extract(pattern, flags=re.I)\n",
        "top_domains = domains.value_counts().head(5)\n",
        "-----------------------------------------------------\n",
        "# Extracting URL Parts Using Multiple Capture Groups\n",
        "# `test_urls` is available from the previous screen\n",
        "pattern = r\"(https?)://([\\w\\.\\-]+)/?(.*)\"\n",
        "\n",
        "test_url_parts = test_urls.str.extract(pattern, flags=re.I)\n",
        "url_parts = hn['url'].str.extract(pattern, flags=re.I)\n",
        "------------------------------------------------------\n",
        "# Using Named Capture Groups to Extract Data\n",
        "# pattern = r\"(https?)://([\\w\\.\\-]+)/?(.*)\"\n",
        "pattern = r\"(?P<protocol>https?)://(?P<domain>[\\w\\.\\-]+)/?(?P<path>.*)\"\n",
        "url_parts = hn['url'].str.extract(pattern, flags=re.I)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0NiCPc78JDr"
      },
      "source": [
        "# List Comprehensions and Lambda Functions\n",
        "\n",
        "# The JSON Format\n",
        "world_cup_str = \"\"\"\n",
        "[\n",
        "    {\n",
        "        \"team_1\": \"France\",\n",
        "        \"team_2\": \"Croatia\",\n",
        "        \"game_type\": \"Final\",\n",
        "        \"score\" : [4, 2]\n",
        "    },\n",
        "    {\n",
        "        \"team_1\": \"Belgium\",\n",
        "        \"team_2\": \"England\",\n",
        "        \"game_type\": \"3rd/4th Playoff\",\n",
        "        \"score\" : [2, 0]\n",
        "    }\n",
        "]\n",
        "\"\"\"\n",
        "import json\n",
        "world_cup_obj = json.loads(world_cup_str)\n",
        "--------------------------------------------------------\n",
        "# Reading a JSON file\n",
        "file = open(\"hn_2014.json\")\n",
        "hn = json.load(file)\n",
        "-----------------------------------------------\n",
        "# Deleting Dictionary Keys\n",
        "def del_key(dict_, key):\n",
        "    # create a copy so we don't\n",
        "    # modify the original dict\n",
        "    modified_dict = dict_.copy()\n",
        "    del modified_dict[key]\n",
        "    return modified_dict\n",
        "hn_clean = []\n",
        "\n",
        "for d in hn:\n",
        "    new_d = del_key(d, 'createdAtI')\n",
        "    hn_clean.append(new_d)\n",
        "--------------------------------------------\n",
        "# Writing List Comprehensions\n",
        "# LOOP VERSION\n",
        "#\n",
        "# hn_clean = []\n",
        "#\n",
        "# for d in hn:\n",
        "#     new_d = del_key(d, 'createdAtI')\n",
        "#     hn_clean.append(new_d)\n",
        "hn_clean = [del_key(d, 'createdAtI') for d in hn]\n",
        "---------------------------------------------------\n",
        "# Using List Comprehensions to Transform and Create Lists\n",
        "urls = [d['url'] for d in hn_clean]\n",
        "--------------------------------------------------\n",
        "# Using List Comprehensions to Reduce a List\n",
        "thousand_points = [d for d in hn_clean if d['points'] > 1000]\n",
        "num_thousand_points = len(thousand_points)\n",
        "------------------------------------------------\n",
        "# Passing Functions as Arguments\n",
        "def get_num_comments(story):\n",
        "    return story['numComments']\n",
        "\n",
        "most_comments = max(hn_clean, key=get_num_comments)\n",
        "-----------------------------------------------\n",
        "# Lambda Functions\n",
        "# def multiply(a, b):\n",
        "#    return a * b\n",
        "multiply = lambda a, b: a * b\n",
        "---------------------------------------------\n",
        "# Using Lambda Functions to Analyze JSON data\n",
        "hn_sorted_points = sorted(hn_clean, key=lambda d: d['points'], reverse=True)\n",
        "top_5_titles = [d['title'] for d in hn_sorted_points[:5]]\n",
        "--------------------------------------------\n",
        "# Reading JSON files into pandas\n",
        "import pandas as pd\n",
        "\n",
        "hn_df = pd.DataFrame(hn_clean)\n",
        "--------------------------------------------\n",
        "# Exploring Tags Using the Apply Function\n",
        "tags = hn_df['tags']\n",
        "has_four_tags = tags.apply(len) == 4\n",
        "four_tags = tags[has_four_tags]\n",
        "--------------------------------------------------\n",
        "# Extracting Tags Using Apply with a Lambda Function\n",
        "# def extract_tag(l):\n",
        "#     return l[-1] if len(l) == 4 else None\n",
        "cleaned_tags = tags.apply(lambda l: l[-1] if len(l) == 4 else None)\n",
        "hn_df['tags'] = cleaned_tags"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUqivI0d-Nk_"
      },
      "source": [
        "# Working with Missing Data\n",
        "----------------------------------------\n",
        "# Introduction\n",
        "import pandas as pd\n",
        "mvc = pd.read_csv(\"nypd_mvc_2018.csv\")\n",
        "null_counts = mvc.isnull().sum()\n",
        "--------------------------------------\n",
        "# Verifying the Total Columns\n",
        "killed_cols = [col for col in mvc.columns if 'killed' in col]\n",
        "killed = mvc[killed_cols].copy()\n",
        "killed_manual_sum = killed.iloc[:,:3].sum(axis=1)\n",
        "killed_mask = killed_manual_sum != killed['total_killed']\n",
        "killed_non_eq = killed[killed_mask]\n",
        "-------------------------------------------\n",
        "# Filling and Verifying the Killed and Injured Data\n",
        "import numpy as np\n",
        "\n",
        "# fix the killed values\n",
        "killed['total_killed'] = killed['total_killed'].mask(killed['total_killed'].isnull(), killed_manual_sum)\n",
        "killed['total_killed'] = killed['total_killed'].mask(killed['total_killed'] != killed_manual_sum, np.nan)\n",
        "\n",
        "# Create an injured dataframe and manually sum values\n",
        "injured = mvc[[col for col in mvc.columns if 'injured' in col]].copy()\n",
        "injured_manual_sum = injured.iloc[:,:3].sum(axis=1)\n",
        "injured['total_injured'] = injured['total_injured'].mask(injured['total_injured'].isnull(), injured_manual_sum)\n",
        "injured['total_injured'] = injured['total_injured'].mask(injured['total_injured'] != injured_manual_sum, np.nan)\n",
        "---------------------------------------------------\n",
        "# Assigning the Corrected Data Back to the Main Dataframe\n",
        "mvc['total_injured'] = injured['total_injured']\n",
        "mvc['total_killed'] = killed['total_killed']\n",
        "----------------------------------------------\n",
        "# Visualizing Missing Data with Plots\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def plot_null_correlations(df):\n",
        "    # create a correlation matrix only for columns with at least\n",
        "    # one missing value\n",
        "    cols_with_missing_vals = df.columns[df.isnull().sum() > 0]\n",
        "    missing_corr = df[cols_with_missing_vals].isnull().corr()\n",
        "    \n",
        "    # create a mask to avoid repeated values and make\n",
        "    # the plot easier to read\n",
        "    missing_corr = missing_corr.iloc[1:, :-1]\n",
        "    mask = np.triu(np.ones_like(missing_corr), k=1)\n",
        "    \n",
        "    # plot a heatmap of the values\n",
        "    plt.figure(figsize=(20,14))\n",
        "    ax = sns.heatmap(missing_corr, vmin=-1, vmax=1, cbar=False,\n",
        "                     cmap='RdBu', mask=mask, annot=True)\n",
        "    \n",
        "    # format the text in the plot to make it easier to read\n",
        "    for text in ax.texts:\n",
        "        t = float(text.get_text())\n",
        "        if -0.05 < t < 0.01:\n",
        "            text.set_text('')\n",
        "        else:\n",
        "            text.set_text(round(t, 2))\n",
        "        text.set_fontsize('x-large')\n",
        "    plt.xticks(rotation=90, size='x-large')\n",
        "    plt.yticks(rotation=0, size='x-large')\n",
        "\n",
        "    plt.show()\n",
        "veh_cols = [c for c in mvc.columns if 'vehicle' in c]\n",
        "plot_null_correlations(mvc[veh_cols])\n",
        "--------------------------------------------------\n",
        "# Analyzing Correlations in Missing Data\n",
        "col_labels = ['v_number', 'vehicle_missing', 'cause_missing']\n",
        "\n",
        "vc_null_data = []\n",
        "\n",
        "# for v in range(1,6):\n",
        "#     v_col = 'vehicle_{}'.format(v)\n",
        "#     c_col = 'cause_vehicle_{}'.format(v)\n",
        "for v in range(1,6):\n",
        "    v_col = 'vehicle_{}'.format(v)\n",
        "    c_col = 'cause_vehicle_{}'.format(v)\n",
        "    \n",
        "    v_null = (mvc[v_col].isnull() & mvc[c_col].notnull()).sum()\n",
        "    c_null = (mvc[c_col].isnull() & mvc[v_col].notnull()).sum()\n",
        "    \n",
        "    vc_null_data.append([v, v_null, c_null])\n",
        "\n",
        "vc_null_df = pd.DataFrame(vc_null_data, columns=col_labels)\n",
        "--------------------------------------------------------\n",
        "# Finding the Most Common Values Across Multiple Columns\n",
        "v_cols = [c for c in mvc.columns if c.startswith(\"vehicle\")]\n",
        "vehicles = mvc[v_cols]\n",
        "vehicles_1d = vehicles.stack()\n",
        "vehicles_counts = vehicles_1d.value_counts()\n",
        "top10_vehicles = vehicles_counts.head(10)\n",
        "---------------------------------------------\n",
        "# Filling Unknown Values with a Placeholder\n",
        "def summarize_missing():\n",
        "    v_missing_data = []\n",
        "\n",
        "    for v in range(1,6):\n",
        "        v_col = 'vehicle_{}'.format(v)\n",
        "        c_col = 'cause_vehicle_{}'.format(v)\n",
        "\n",
        "        v_missing = (mvc[v_col].isnull() & mvc[c_col].notnull()).sum()\n",
        "        c_missing = (mvc[c_col].isnull() & mvc[v_col].notnull()).sum()\n",
        "\n",
        "        v_missing_data.append([v, v_missing, c_missing])\n",
        "\n",
        "    col_labels = columns=[\"vehicle_number\", \"vehicle_missing\", \"cause_missing\"]\n",
        "    return pd.DataFrame(v_missing_data, columns=col_labels)\n",
        "\n",
        "summary_before = summarize_missing()\n",
        "\n",
        "# for v in range(1,6):\n",
        "#     v_col = 'vehicle_{}'.format(v)\n",
        "#     c_col = 'cause_vehicle_{}'.format(v)\n",
        "for v in range(1,6):\n",
        "    v_col = 'vehicle_{}'.format(v)\n",
        "    c_col = 'cause_vehicle_{}'.format(v)\n",
        "    \n",
        "    v_missing_mask = mvc[v_col].isnull() & mvc[c_col].notnull()\n",
        "    c_missing_mask = mvc[c_col].isnull() & mvc[v_col].notnull()\n",
        "\n",
        "    mvc[v_col] = mvc[v_col].mask(v_missing_mask, \"Unspecified\")\n",
        "    mvc[c_col] = mvc[c_col].mask(c_missing_mask, \"Unspecified\")\n",
        "\n",
        "summary_after = summarize_missing()\n",
        "---------------------------------------------------\n",
        "# Imputing Location Data\n",
        "sup_data = pd.read_csv('supplemental_data.csv')\n",
        "\n",
        "location_cols = ['location', 'on_street', 'off_street', 'borough']\n",
        "null_before = mvc[location_cols].isnull().sum()\n",
        "for col in location_cols:\n",
        "    mvc[col] = mvc[col].mask(mvc[col].isnull(), sup_data[col])\n",
        "\n",
        "null_after = mvc[location_cols].isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gllDsen82r4G"
      },
      "source": [
        "# Data Cleaning Walkthrough\n",
        "\n",
        "# Reading in the Data\n",
        "import pandas as pd\n",
        "data_files = [\n",
        "    \"ap_2010.csv\",\n",
        "    \"class_size.csv\",\n",
        "    \"demographics.csv\",\n",
        "    \"graduation.csv\",\n",
        "    \"hs_directory.csv\",\n",
        "    \"sat_results.csv\"\n",
        "]\n",
        "data = {}\n",
        "for f in data_files:\n",
        "    d = pd.read_csv(\"schools/{0}\".format(f))\n",
        "    key_name = f.replace(\".csv\", \"\")\n",
        "    data[key_name] = d\n",
        "---------------------------------------------    \n",
        "# Exploring the SAT Data\n",
        "print(data[\"sat_results\"].head())\n",
        "---------------------------------------\n",
        "# Exploring the Remaining Data\n",
        "for k in data:\n",
        "    print(data[k].head())\n",
        "-----------------------------------------    \n",
        "# Reading in the Survey Data\n",
        "all_survey = pd.read_csv(\"schools/survey_all.txt\", delimiter=\"\\t\", encoding='windows-1252')\n",
        "d75_survey = pd.read_csv(\"schools/survey_d75.txt\", delimiter=\"\\t\", encoding='windows-1252')\n",
        "survey = pd.concat([all_survey, d75_survey], axis=0)\n",
        "\n",
        "print(survey.head())\n",
        "--------------------------------------------------\n",
        "# Cleaning Up the Surveys\n",
        "survey[\"DBN\"] = survey[\"dbn\"]\n",
        "\n",
        "survey_fields = [\n",
        "    \"DBN\", \n",
        "    \"rr_s\", \n",
        "    \"rr_t\", \n",
        "    \"rr_p\", \n",
        "    \"N_s\", \n",
        "    \"N_t\", \n",
        "    \"N_p\", \n",
        "    \"saf_p_11\", \n",
        "    \"com_p_11\", \n",
        "    \"eng_p_11\", \n",
        "    \"aca_p_11\", \n",
        "    \"saf_t_11\", \n",
        "    \"com_t_11\", \n",
        "    \"eng_t_11\", \n",
        "    \"aca_t_11\", \n",
        "    \"saf_s_11\", \n",
        "    \"com_s_11\", \n",
        "    \"eng_s_11\", \n",
        "    \"aca_s_11\", \n",
        "    \"saf_tot_11\", \n",
        "    \"com_tot_11\", \n",
        "    \"eng_tot_11\", \n",
        "    \"aca_tot_11\",\n",
        "]\n",
        "survey = survey.loc[:,survey_fields]\n",
        "data[\"survey\"] = survey\n",
        "\n",
        "print(survey.head())\n",
        "------------------------------------------------\n",
        "# Inserting DBN Fields\n",
        "data[\"hs_directory\"][\"DBN\"] = data[\"hs_directory\"][\"dbn\"]\n",
        "\n",
        "def pad_csd(num):\n",
        "    return str(num).zfill(2)\n",
        "    \n",
        "data[\"class_size\"][\"padded_csd\"] = data[\"class_size\"][\"CSD\"].apply(pad_csd)\n",
        "data[\"class_size\"][\"DBN\"] = data[\"class_size\"][\"padded_csd\"] + data[\"class_size\"][\"SCHOOL CODE\"]\n",
        "print(data[\"class_size\"].head())\n",
        "----------------------------------------------------\n",
        "# Combining the SAT Scores\n",
        "cols = ['SAT Math Avg. Score', 'SAT Critical Reading Avg. Score', 'SAT Writing Avg. Score']\n",
        "for c in cols:\n",
        "    data[\"sat_results\"][c] = pd.to_numeric(data[\"sat_results\"][c], errors=\"coerce\")\n",
        "\n",
        "data['sat_results']['sat_score'] = data['sat_results'][cols[0]] + data['sat_results'][cols[1]] + data['sat_results'][cols[2]]\n",
        "print(data['sat_results']['sat_score'].head())\n",
        "------------------------------------------------------\n",
        "# Parsing Geographic Coordinates for Schools\n",
        "import re\n",
        "def find_lat(loc):\n",
        "    coords = re.findall(\"\\(.+\\)\", loc)\n",
        "    lat = coords[0].split(\",\")[0].replace(\"(\", \"\")\n",
        "    return lat\n",
        "\n",
        "data[\"hs_directory\"][\"lat\"] = data[\"hs_directory\"][\"Location 1\"].apply(find_lat)\n",
        "\n",
        "print(data[\"hs_directory\"].head())\n",
        "---------------------------------------------------------------\n",
        "# Extracting the Longitude\n",
        "import re\n",
        "def find_lon(loc):\n",
        "    coords = re.findall(\"\\(.+\\)\", loc)\n",
        "    lon = coords[0].split(\",\")[1].replace(\")\", \"\").strip()\n",
        "    return lon\n",
        "\n",
        "data[\"hs_directory\"][\"lon\"] = data[\"hs_directory\"][\"Location 1\"].apply(find_lon)\n",
        "\n",
        "data[\"hs_directory\"][\"lat\"] = pd.to_numeric(data[\"hs_directory\"][\"lat\"], errors=\"coerce\")\n",
        "data[\"hs_directory\"][\"lon\"] = pd.to_numeric(data[\"hs_directory\"][\"lon\"], errors=\"coerce\")\n",
        "\n",
        "print(data[\"hs_directory\"].head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaPvfjSv4Te0"
      },
      "source": [
        "# Data Cleaning Walkthrough: Combining the Data\n",
        "\n",
        "# Condensing the Class Size Data Set\n",
        "class_size = data[\"class_size\"]\n",
        "class_size = class_size[class_size[\"GRADE \"] == \"09-12\"]\n",
        "class_size = class_size[class_size[\"PROGRAM TYPE\"] == \"GEN ED\"]\n",
        "print(class_size.head())\n",
        "---------------------------------------------------\n",
        "# Computing Average Class Sizes\n",
        "import numpy\n",
        "class_size = class_size.groupby(\"DBN\").agg(numpy.mean)\n",
        "class_size.reset_index(inplace=True)\n",
        "data[\"class_size\"] = class_size\n",
        "print(data[\"class_size\"].head())\n",
        "--------------------------------------------------\n",
        "# Condensing the Demographics Data Set\n",
        "data[\"demographics\"] = data[\"demographics\"][data[\"demographics\"][\"schoolyear\"] == 20112012]\n",
        "print(data[\"demographics\"].head())\n",
        "-------------------------------------------------\n",
        "# Condensing the Graduation Data Set\n",
        "data[\"graduation\"] = data[\"graduation\"][data[\"graduation\"][\"Cohort\"] == \"2006\"]\n",
        "data[\"graduation\"] = data[\"graduation\"][data[\"graduation\"][\"Demographic\"] == \"Total Cohort\"]\n",
        "print(data[\"graduation\"].head())\n",
        "------------------------------------------------\n",
        "# Converting AP Test Scores\n",
        "cols = ['AP Test Takers ', 'Total Exams Taken', 'Number of Exams with scores 3 4 or 5']\n",
        "for col in cols:\n",
        "    data[\"ap_2010\"][col] = pd.to_numeric(data[\"ap_2010\"][col], errors=\"coerce\")\n",
        "    \n",
        "print(data[\"ap_2010\"].dtypes)\n",
        "-------------------------------------------------\n",
        "# Performing the Left Joins\n",
        "combined = data[\"sat_results\"]\n",
        "combined = combined.merge(data[\"ap_2010\"], on=\"DBN\", how=\"left\")\n",
        "combined = combined.merge(data[\"graduation\"], on=\"DBN\", how=\"left\")\n",
        "print(combined.head(5))\n",
        "print(combined.shape)\n",
        "-----------------------------------------------------\n",
        "# Performing the Inner Joins\n",
        "to_merge = [\"class_size\", \"demographics\", \"survey\", \"hs_directory\"]\n",
        "\n",
        "for m in to_merge:\n",
        "    combined = combined.merge(data[m], on=\"DBN\", how=\"inner\")\n",
        "\n",
        "print(combined.head(5))\n",
        "print(combined.shape)\n",
        "--------------------------------------------------------\n",
        "# Filling in Missing Values\n",
        "combined = combined.fillna(combined.mean())\n",
        "combined = combined.fillna(0)\n",
        "\n",
        "print(combined.head(5))\n",
        "------------------------------------------------------\n",
        "# 16.Adding a School District Column for Mapping\n",
        "def get_first_two_chars(dbn):\n",
        "    return dbn[0:2]\n",
        "\n",
        "combined[\"school_dist\"] = combined[\"DBN\"].apply(get_first_two_chars)\n",
        "print(combined[\"school_dist\"].head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFOVwVY5524h"
      },
      "source": [
        "# Data Cleaning Walkthrough: Analyzing and Visualizing the Data\n",
        "\n",
        "# Finding Correlations With the r Value\n",
        "correlations = combined.corr()\n",
        "correlations = correlations[\"sat_score\"]\n",
        "print(correlations)\n",
        "--------------------------------------------\n",
        "# Plotting Enrollment With the Plot() Accessor\n",
        "import matplotlib.pyplot as plt\n",
        "combined.plot.scatter(x='total_enrollment', y='sat_score')\n",
        "plt.show()\n",
        "-------------------------------------------------\n",
        "# Exploring Schools With Low SAT Scores and Enrollment\n",
        "low_enrollment = combined[combined[\"total_enrollment\"] < 1000]\n",
        "low_enrollment = low_enrollment[low_enrollment[\"sat_score\"] < 1000]\n",
        "print(low_enrollment[\"School Name\"])\n",
        "----------------------------------------------\n",
        "# Plotting Language Learning Percentage\n",
        "combined.plot.scatter(x='ell_percent', y='sat_score')\n",
        "plt.show()\n",
        "-----------------------------------------------\n",
        "# Calculating District-Level Statistics\n",
        "import numpy\n",
        "districts = combined.groupby(\"school_dist\").agg(numpy.mean)\n",
        "districts.reset_index(inplace=True)\n",
        "print(districts.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-esSHapL6r9t"
      },
      "source": [
        "# Challenge: Cleaning Data\n",
        "\n",
        "# Exploring the Data\n",
        "import pandas as pd\n",
        "\n",
        "avengers = pd.read_csv(\"avengers.csv\")\n",
        "avengers.head(5)\n",
        "--------------------------------\n",
        "# Filtering Out Bad Data\n",
        "import matplotlib.pyplot as plt\n",
        "true_avengers = pd.DataFrame()\n",
        "\n",
        "avengers['Year'].hist()\n",
        "true_avengers = avengers[avengers[\"Year\"] > 1959]\n",
        "------------------------------------\n",
        "# Consolidating Deaths\n",
        "def clean_deaths(row):\n",
        "    num_deaths = 0\n",
        "    columns = ['Death1', 'Death2', 'Death3', 'Death4', 'Death5']\n",
        "    \n",
        "    for c in columns:\n",
        "        death = row[c]\n",
        "        if pd.isnull(death) or death == 'NO':\n",
        "            continue\n",
        "        elif death == 'YES':\n",
        "            num_deaths += 1\n",
        "    return num_deaths\n",
        "\n",
        "true_avengers['Deaths'] = true_avengers.apply(clean_deaths, axis=1)\n",
        "--------------------------------------------\n",
        "# Verifying Years Since Joining\n",
        "joined_accuracy_count  = int()\n",
        "correct_joined_years = true_avengers[true_avengers['Years since joining'] == (2015 - true_avengers['Year'])]\n",
        "joined_accuracy_count = len(correct_joined_years)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9L563WW-iiZT"
      },
      "source": [
        "#Sampling\n",
        "\n",
        "#Populations and Samples\n",
        "question1 = ''\n",
        "question2 = ''\n",
        "question3 = ''\n",
        "question4 = ''\n",
        "question5 = ''\n",
        "question1 = 'population'\n",
        "question2 = 'population'\n",
        "question3 = 'sample'\n",
        "question4 = 'population'\n",
        "question5 = 'sample'\n",
        "-------------------------------------------------\n",
        "#Sampling Error\n",
        "import pandas as pd\n",
        "wnba = pd.read_csv('wnba.csv')\n",
        "parameter = wnba['Games Played'].max()\n",
        "sample = wnba['Games Played'].sample(30, random_state = 1)\n",
        "statistic = sample.max()\n",
        "sampling_error = parameter - statistic\n",
        "----------------------------------------------------\n",
        "#Simple Random Sampling\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "wnba = pd.read_csv('wnba.csv')\n",
        "sample_means = []\n",
        "population_mean = wnba['PTS'].mean()\n",
        "\n",
        "for i in range(100):\n",
        "    sample = wnba['PTS'].sample(10, random_state=i)\n",
        "    sample_means.append(sample.mean())\n",
        "\n",
        "plt.scatter(range(1,101), sample_means)\n",
        "plt.axhline(population_mean)\n",
        "-------------------------------------------------\n",
        "#Stratified Sampling\n",
        "wnba['Pts_per_game'] = wnba['PTS'] / wnba['Games Played']\n",
        "\n",
        "# Stratifying the data in five strata\n",
        "stratum_G = wnba[wnba.Pos == 'G']\n",
        "stratum_F = wnba[wnba.Pos == 'F']\n",
        "stratum_C = wnba[wnba.Pos == 'C']\n",
        "stratum_GF = wnba[wnba.Pos == 'G/F']\n",
        "stratum_FC = wnba[wnba.Pos == 'F/C']\n",
        "\n",
        "points_per_position = {}\n",
        "for stratum, position in [(stratum_G, 'G'), (stratum_F, 'F'), (stratum_C, 'C'),\n",
        "                (stratum_GF, 'G/F'), (stratum_FC, 'F/C')]:\n",
        "    \n",
        "    sample = stratum['Pts_per_game'].sample(10, random_state = 0) # simple random sampling on each stratum\n",
        "    points_per_position[position] = sample.mean()\n",
        "    \n",
        "position_most_points = max(points_per_position, key = points_per_position.get)\n",
        "------------------------------------------------------\n",
        "#Proportional Stratified Sampling\n",
        "under_12 = wnba[wnba['Games Played'] <= 12]\n",
        "btw_13_22 = wnba[(wnba['Games Played'] > 12) & (wnba['Games Played'] <= 22)]\n",
        "over_23 = wnba[wnba['Games Played'] > 22]\n",
        "\n",
        "proportional_sampling_means = []\n",
        "\n",
        "for i in range(100):\n",
        "    sample_under_12 = under_12['PTS'].sample(1, random_state = i)\n",
        "    sample_btw_13_22 = btw_13_22['PTS'].sample(2, random_state = i)\n",
        "    sample_over_23 = over_23['PTS'].sample(7, random_state = i)\n",
        "    \n",
        "    final_sample = pd.concat([sample_under_12, sample_btw_13_22, sample_over_23])\n",
        "    proportional_sampling_means.append(final_sample.mean())\n",
        "    \n",
        "plt.scatter(range(1,101), proportional_sampling_means)\n",
        "plt.axhline(wnba['PTS'].mean())\n",
        "---------------------------------------------------\n",
        "#Cluster Sampling\n",
        "clusters = pd.Series(wnba['Team'].unique()).sample(4, random_state = 0)\n",
        "\n",
        "sample = pd.DataFrame()\n",
        "\n",
        "for cluster in clusters:\n",
        "    data_collected = wnba[wnba['Team'] == cluster]\n",
        "    sample = sample.append(data_collected)\n",
        "\n",
        "sampling_error_height = wnba['Height'].mean() - sample['Height'].mean()\n",
        "sampling_error_age = wnba['Age'].mean() - sample['Age'].mean()\n",
        "sampling_error_BMI = wnba['BMI'].mean() - sample['BMI'].mean()\n",
        "sampling_error_points = wnba['PTS'].mean() - sample['PTS'].mean()\n",
        "-----------------------------------------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMuTG3mcj_1F"
      },
      "source": [
        "#Variables in Statistics\n",
        "\n",
        "#Quantitative and Qualitative Variables\n",
        "import pandas as pd\n",
        "wnba = pd.read_csv('wnba.csv')\n",
        "\n",
        "variables = {'Name': '', 'Team': '', 'Pos': '', 'Height': '', 'BMI': '',\n",
        "             'Birth_Place': '', 'Birthdate': '', 'Age': '', 'College': '', 'Experience': '',\n",
        "             'Games Played': '', 'MIN': '', 'FGM': '', 'FGA': '',\n",
        "             '3PA': '', 'FTM': '', 'FTA': '', 'FT%': '', 'OREB': '', 'DREB': '',\n",
        "             'REB': '', 'AST': '', 'PTS': ''}\n",
        "variables = {'Name': 'qualitative', 'Team': 'qualitative', 'Pos': 'qualitative',\n",
        "             'Height': 'quantitative', 'BMI': 'quantitative',\n",
        "             'Birth_Place': 'qualitative', 'Birthdate': 'quantitative', 'Age': 'quantitative', \n",
        "             'College': 'qualitative', 'Experience': 'quantitative', 'Games Played': 'quantitative',\n",
        "             'MIN': 'quantitative', 'FGM': 'quantitative', 'FGA': 'quantitative',\n",
        "             '3PA': 'quantitative', 'FTM': 'quantitative',\n",
        "             'FTA': 'quantitative', 'FT%': 'quantitative', 'OREB': 'quantitative', 'DREB': 'quantitative',\n",
        "             'REB': 'quantitative', 'AST': 'quantitative', 'PTS': 'quantitative'}\n",
        "------------------------------------------------------------\n",
        "#The Normal  Scale\n",
        "nominal_scale = sorted(['Name', 'Team', 'Pos', 'Birth_Place', 'College'])\n",
        "----------------------------------------------------------------------\n",
        "#The Ordinal Scale\n",
        "question1 = True\n",
        "question2 = False\n",
        "question3 = False\n",
        "question4 = True\n",
        "question5 = False\n",
        "question6 = False\n",
        "-----------------------------------------------------------\n",
        "#The Difference Between Ratio and Interval Scales\n",
        "interval = ['Birthdate', 'Weight_deviation']\n",
        "ratio = sorted(['Height', 'Weight', 'BMI', 'Age', 'Experience', 'Games Played', 'MIN', 'FGM', 'FGA', 'FG%', '15:00', \n",
        "                '3PA', '3P%', 'FTM', 'FTA', 'FT%', 'OREB', 'DREB', 'REB', 'AST', 'STL', 'BLK', 'TO',\n",
        "                'PTS', 'DD2', 'TD3'])\n",
        "-------------------------------------------------------------\n",
        "#Discrete and Continuous Variables\n",
        "ratio_interval_only = {'Height':'', 'Weight': '', 'BMI': '', 'Age': '', 'Games Played': '', 'MIN': '', 'FGM': '',\n",
        "                       'FGA': '', 'FG%': '', '3PA': '', '3P%': '', 'FTM': '', 'FTA': '', 'FT%': '',\n",
        "                       'OREB': '', 'DREB': '', 'REB': '', 'AST': '', 'STL': '', 'BLK': '', 'TO': '',\n",
        "                       'PTS': '', 'DD2': '', 'TD3': '', 'Weight_deviation': ''}\n",
        "ratio_interval_only = {'Height': 'continuous', 'Weight': 'continuous', 'BMI': 'continuous', 'Age': 'continuous',\n",
        "                       'Games Played': 'discrete', 'MIN': 'continuous', 'FGM': 'discrete',\n",
        "                       'FGA': 'discrete', 'FG%': 'continuous', '3PA': 'discrete', '3P%': 'continuous',\n",
        "                       'FTM': 'discrete', 'FTA': 'discrete', 'FT%': 'continuous', 'OREB': 'discrete',\n",
        "                       'DREB': 'discrete', 'REB': 'discrete', 'AST': 'discrete', 'STL': 'discrete',\n",
        "                       'BLK': 'discrete', 'TO': 'discrete', 'PTS': 'discrete', 'DD2': 'discrete', \n",
        "                       'TD3': 'discrete', 'Weight_deviation': 'continuous'}\n",
        "---------------------------------------------------------------------------                       \n",
        "#Real Limits\n",
        "bmi = {21.201: [],\n",
        " 21.329: [],\n",
        " 23.875: [],\n",
        " 24.543: [],\n",
        " 25.469: []}\n",
        "bmi = {21.201: [21.2005, 21.2015],\n",
        " 21.329: [21.3285, 21.3295],\n",
        " 23.875: [23.8745, 23.8755],\n",
        " 24.543: [24.5425, 24.5435],\n",
        " 25.469: [25.4685, 25.4695]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOChFPxBlK2X"
      },
      "source": [
        "#Frequency Distributions\n",
        "\n",
        "#Frequency Distribution Tables\n",
        "wnba = pd.read_csv('wnba.csv')\n",
        "freq_distro_pos = wnba['Pos'].value_counts()\n",
        "freq_distro_height = wnba['Height'].value_counts()\n",
        "-------------------------------------------------------\n",
        "#Sorting Frequency Distribution Tables\n",
        "wnba = pd.read_csv('wnba.csv')\n",
        "age_ascending = wnba['Age'].value_counts().sort_index()\n",
        "age_descending = wnba['Age'].value_counts().sort_index(ascending = False)\n",
        "-------------------------------------------------------\n",
        "#Sorting Tables for Ordinal Variables\n",
        "def make_pts_ordinal(row):\n",
        "    if row['PTS'] <= 20:\n",
        "        return 'very few points'\n",
        "    if (20 < row['PTS'] <=  80):\n",
        "        return 'few points'\n",
        "    if (80 < row['PTS'] <=  150):\n",
        "        return 'many, but below average'\n",
        "    if (150 < row['PTS'] <= 300):\n",
        "        return 'average number of points'\n",
        "    if (300 < row['PTS'] <=  450):\n",
        "        return 'more than average'\n",
        "    else:\n",
        "        return 'much more than average'\n",
        "    \n",
        "wnba['PTS_ordinal_scale'] = wnba.apply(make_pts_ordinal, axis = 1)\n",
        "\n",
        "# Type your answer below\n",
        "pts_ordinal_desc = wnba['PTS_ordinal_scale'].value_counts().iloc[[4, 3, 0, 2, 1, 5]]\n",
        "------------------------------------------------------------------\n",
        "#Proportions and Percentages\n",
        "wnba = pd.read_csv('wnba.csv')\n",
        "percentages = wnba['Age'].value_counts(normalize = True).sort_index() * 100\n",
        "proportion_25 = percentages[25] / 100\n",
        "percentage_30 = percentages[30]\n",
        "percentage_over_30 = percentages.loc[30:].sum()\n",
        "percentage_below_23 = percentages.loc[:23].sum()\n",
        "----------------------------------------------------------\n",
        "#Percentiles and Percentile Ranks\n",
        "wnba = pd.read_csv('wnba.csv')\n",
        "from scipy.stats import percentileofscore\n",
        "percentile_rank_half_less = percentileofscore(wnba['Games Played'], 17, kind = 'weak')\n",
        "percentage_half_more = 100 - percentile_rank_half_less\n",
        "--------------------------------------------------------------\n",
        "#Finding Percentiles with pandas\n",
        "wnba = pd.read_csv('wnba.csv')\n",
        "percentiles = wnba['Age'].describe(percentiles = [.5, .75, .95])\n",
        "age_upper_quartile = percentiles['75%']\n",
        "age_middle_quartile = percentiles['50%']\n",
        "age_95th_percentile = percentiles['95%']\n",
        "\n",
        "question1 = True\n",
        "question2 = False\n",
        "question3 = True\n",
        "--------------------------------------------------------------\n",
        "#Grouped Frequency Distribution Tables\n",
        "wnba = pd.read_csv('wnba.csv')\n",
        "grouped_freq_table = wnba['PTS'].value_counts(bins = 10,\n",
        "                normalize = True).sort_index(ascending = False) * 100\n",
        "---------------------------------------------------------------------------\n",
        "#Readability for Grouped Frequency Tables\n",
        "wnba = pd.read_csv('wnba.csv')\n",
        "intervals = pd.interval_range(start = 0, end = 600, freq = 60)\n",
        "gr_freq_table_10 = pd.Series([0 for _ in range(10)], index = intervals)\n",
        "\n",
        "for value in wnba['PTS']:\n",
        "    for interval in intervals:\n",
        "        if value in interval:\n",
        "            gr_freq_table_10.loc[interval] += 1\n",
        "            break\n",
        "-------------------------------------------------------------            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ_x2DSnm80g"
      },
      "source": [
        "####Importnat Data Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgJjTmFJm07i"
      },
      "source": [
        "#Visualizing Frequency Distributions\n",
        "\n",
        "#Bar Plots\n",
        "wnba['Exp_ordinal'].value_counts().iloc[[3,0,2,1,4]].plot.bar()\n",
        "----------------------------------------------\n",
        "#Horizontal Bar Plots\n",
        "wnba['Exp_ordinal'].value_counts().iloc[[3,0,2,1,4]].plot.barh(\n",
        "    title = 'Number of players in WNBA by level of experience')\n",
        "----------------------------------------------\n",
        "#Pie Charts\n",
        "wnba['Exp_ordinal'].value_counts().plot.pie()\n",
        "----------------------------------------------------\n",
        "#Customizing a Pie Chart\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "wnba['Exp_ordinal'].value_counts().plot.pie(figsize = (6,6), autopct = '%.2f%%',\n",
        "                                    title = 'Percentage of players in WNBA by level of experience')\n",
        "plt.ylabel('')\n",
        "------------------------------------------------------------\n",
        "#Histograms\n",
        "wnba['PTS'].plot.hist()\n",
        "--------------------------------------------------\n",
        "#The Statistics Behind Histograms\n",
        "wnba['Games Played'].plot.hist()\n",
        "-----------------------------------------------\n",
        "#Binning for Histograms\n",
        "wnba['Games Played'].plot.hist(range = (1,32), bins = 8,\n",
        "                               title = 'The distribution of players by games played')\n",
        "plt.xlabel('Games played')\n",
        "----------------------------------------------------\n",
        "#Skewed Distributions\n",
        "assists_distro = 'right skewed'\n",
        "ft_percent_distro = 'left skewed'\n",
        "---------------------------------------\n",
        "#Symmetrical Distributions\n",
        "normal_distribution = 'Height'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhrxfTr4oNgT"
      },
      "source": [
        "#Comparing Frequency Distributions\n",
        "\n",
        "#Grouped Bar Plots\n",
        "import seaborn as sns\n",
        "sns.countplot(x = 'Exp_ordinal', hue = 'Pos', data = wnba,\n",
        "              order = ['Rookie', 'Little experience', 'Experienced', 'Very experienced', 'Veteran'],\n",
        "              hue_order = ['C', 'F', 'F/C', 'G', 'G/F']\n",
        "             )\n",
        "------------------------------------------------\n",
        "#Challenge: Do Older Players Play Less?\n",
        "sns.countplot(x = 'age_mean_relative', hue = 'min_mean_relative', data = wnba)\n",
        "result = 'rejection'\n",
        "--------------------------------------------------\n",
        "#Comparing Histograms\n",
        "import matplotlib.pyplot as plt\n",
        "wnba[wnba.Age >= 27]['MIN'].plot.hist(histtype = 'step', label = 'Old', legend = True)\n",
        "wnba[wnba.Age < 27]['MIN'].plot.hist(histtype = 'step', label = 'Young', legend = True)\n",
        "plt.axvline(497, label = 'Average')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "------------------------------------------------------------\n",
        "#Kernel Density Estimate Plots\n",
        "wnba[wnba.Age >= 27]['MIN'].plot.kde(label = 'Old', legend = True)\n",
        "wnba[wnba.Age < 27]['MIN'].plot.kde(label = 'Young', legend = True)\n",
        "plt.axvline(497, label = 'Average')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "'''We can still observe that most of the old players that belong to the \"average or above\"\n",
        "category play significantly more than average. With the help of the vertical line, the pattern\n",
        "is very easy to notice. Because the graph looks much cleaner than the one with step-type\n",
        "histograms, we can easily argue that the pattern is much more obvious in the case of kernel density\n",
        "plots.'''\n",
        "-------------------------------------------------------------------\n",
        "#Strip Plots\n",
        "sns.stripplot(x = 'Pos', y = 'Weight', data = wnba, jitter = True)\n",
        "plt.show()\n",
        "\n",
        "'''The patterns we see are strikingly similar to those we saw for heights. This can be easily\n",
        "explained by the fact that there's a strong positive relation between a player's height and her\n",
        "weight: the taller the player, the heavier she is; the shorter the player, the lighter she is.'''\n",
        "------------------------------------------------------------------\n",
        "#Box plots\n",
        "sns.boxplot(x = 'Pos', y = 'Weight', data = wnba)\n",
        "plt.show()\n",
        "---------------------------------------------------------------\n",
        "#Outliers\n",
        "iqr = 29 - 22\n",
        "lower_bound = 22 - (1.5 * iqr)\n",
        "upper_bound = 29 + (1.5 * iqr)\n",
        "outliers_low = sum(wnba['Games Played'] < lower_bound) # True values will count as 1 in the summation\n",
        "outliers_high = sum(wnba['Games Played'] > upper_bound)\n",
        "\n",
        "sns.boxplot(wnba['Games Played'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}